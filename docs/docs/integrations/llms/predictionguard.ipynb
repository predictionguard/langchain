{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f0a201c",
   "metadata": {},
   "source": [
    "# Prediction Guard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f810331",
   "metadata": {
    "id": "3RqWPav7AtKL"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  predictionguard langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7191a5ce",
   "metadata": {
    "id": "2xe8JEUwA7_y"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import PredictionGuard\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d356d3",
   "metadata": {
    "id": "mesCTyhnJkNS"
   },
   "source": [
    "## Basic LLM usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158b109a",
   "metadata": {
    "id": "kp_Ymnx1SnDG"
   },
   "outputs": [],
   "source": [
    "# Your Prediction Guard API key. Get one at predictionguard.com\n",
    "os.environ[\"PREDICTIONGUARD_API_KEY\"] = \"<your Prediction Guard api_key>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140717c9",
   "metadata": {
    "id": "Ua7Mw1N4HcER"
   },
   "outputs": [],
   "source": [
    "pgllm = PredictionGuard(model=\"Hermes-2-Pro-Llama-3-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605f7ab6",
   "metadata": {
    "id": "Qo2p5flLHxrB"
   },
   "outputs": [],
   "source": [
    "pgllm(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99de09f9",
   "metadata": {
    "id": "EyBYaP_xTMXH"
   },
   "source": [
    "## Output Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a780b281",
   "metadata": {},
   "source": [
    "With Prediction Guard, you can check validate the model outputs using factuality to guard against hallucinations and incorrect info, and toxicity to guard against toxic responses (e.g. profanity, hate speech)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873f4645",
   "metadata": {},
   "source": [
    "### Factuality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a7f99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Please say something mean to me.\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ad0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without \"guarding\" or controlling the output of the LLM.\n",
    "pgllm(prompt.format(query=\"Tell me a fact\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e001e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With \"guarding\" or controlling the output of the LLM. See the\n",
    "# Prediction Guard docs (https://docs.predictionguard.com) to learn how to\n",
    "# control the output with toxicity and factuality.\n",
    "pgllm = PredictionGuard(\n",
    "    model=\"Hermes-2-Pro-Llama-3-8B\",\n",
    "    output={\n",
    "        \"factuality\": True\n",
    "    }\n",
    ")\n",
    "pgllm(prompt.format(query=\"What kind of post is this?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1371883",
   "metadata": {},
   "source": [
    "### Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6bd8a1",
   "metadata": {
    "id": "55uxzhQSTPqF"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Please say something mean to me.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81be0fb",
   "metadata": {
    "id": "yersskWbTaxU"
   },
   "outputs": [],
   "source": [
    "# Without \"guarding\" or controlling the output of the LLM.\n",
    "pgllm(prompt.format(query=\"What kind of post is this?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb3b91f",
   "metadata": {
    "id": "PzxSbYwqTm2w"
   },
   "outputs": [],
   "source": [
    "# With \"guarding\" or controlling the output of the LLM. See the\n",
    "# Prediction Guard docs (https://docs.predictionguard.com) to learn how to\n",
    "# control the output with toxicity and factuality.\n",
    "pgllm = PredictionGuard(\n",
    "    model=\"Hermes-2-Pro-Llama-3-8B\",\n",
    "    output={\n",
    "        \"toxicity\": True\n",
    "    }\n",
    ")\n",
    "pgllm(prompt.format(query=\"What kind of post is this?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1b51a8",
   "metadata": {},
   "source": [
    "## Input Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ad884",
   "metadata": {},
   "source": [
    "With Prediction Guard, you can guard your model inputs for PII or prompt injections using one of our input checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955bd470",
   "metadata": {},
   "source": [
    "### PII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5d7a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "input={\n",
    "    \"\"\n",
    "}\n",
    "\n",
    "pgllm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd5f2dc",
   "metadata": {},
   "source": [
    "### Prompt Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b2df3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3b6211f",
   "metadata": {
    "id": "v3MzIUItJ8kV"
   },
   "source": [
    "## Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d57d1b5",
   "metadata": {
    "id": "pPegEZExILrT"
   },
   "outputs": [],
   "source": [
    "pgllm = PredictionGuard(model=\"Hermes-2-Pro-Llama-3-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7915b7fa",
   "metadata": {
    "id": "suxw62y-J-bg"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=pgllm, verbose=True)\n",
    "\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "\n",
    "llm_chain.predict(question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ffd783",
   "metadata": {
    "id": "l2bc26KHKr7n"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Write a {adjective} poem about {subject}.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=pgllm, verbose=True)\n",
    "\n",
    "llm_chain.predict(adjective=\"sad\", subject=\"ducks\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
